# -*- coding: utf-8 -*-
"""Sentiment Analysist with IndoNLU

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KbvZkDPuX2cUELkBf7KUhtTv9EfiMt4a

Menginstall packages PyTorch
"""

pip install torch torchvision

"""# **Analisis Sentimen dengan Deep Learning**"""

import torch

pip install transformers

"""Selanjutnya, clone akun github IndoNLU untuk menyimpan dataset pada storage session Google Colab. Eksekusi kode berikut.

Pada project ini, saya akan menggunakan dataset analisis sentimen dari IndoNLU. Ini merupakan dataset khusus sentimen berbahasa Indonesia yang sebelumnya telah melewati tahap pre-processing. Dataset ini merupakan kumpulan teks atau dokumen berbahasa Indonesia yang diperoleh melalui proses crawling (proses ekstraksi data dari web dengan ukuran yang besar). Beberapa sumber teks antara lain Twitter, Zomato, TripAdvisor, Facebook, Instagram, dan Qraved. Data kemudian dianotasi oleh beberapa ahli bahasa Indonesia.

karena kita akan menggunakan pre-trained model IndoBert, instal juga transformers. Ia adalah library yang menyediakan general-purpose architectures (BERT, GPT-2, XLM, dll) untuk NLU.
"""

!git clone https://github.com/indobenchmark/indonlu

"""Perhatikan folder indonlu > dataset > smsa_doc_sentiment_prosa. Pada folder itulah data kita berada. Terlihat beberapa file seperti train_preprocess.tsv (ini adalah data latih yang akan kita gunakan), dan valid_preprocess.tsv (ini adalah data validasi yang akan kita gunakan). 

Selanjutnya, impor semua library yang dibutuhkan. Tuliskan kode berikut pada sel selanjutnya.
"""

import random
import numpy as np
import pandas as pd
import torch
from torch import optim
import torch.nn.functional as F
from tqdm import tqdm
 
from transformers import BertForSequenceClassification, BertConfig, BertTokenizer
from nltk.tokenize import TweetTokenizer
 
from indonlu.utils.forward_fn import forward_sequence_classification
from indonlu.utils.metrics import document_sentiment_metrics_fn
from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader

"""Selanjutnya, definisikan fungsi umum sebagai berikut:

set_seed : Mengatur dan menetapkan random seed.

count_param : Menghitung jumlah parameter dalam model

get_lr : Mengatur learning rate

metrics_to_string : Mengonversi metriks ke dalam string
"""

###
# common functions
###
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    
def count_param(module, trainable=False):
    if trainable:
        return sum(p.numel() for p in module.parameters() if p.requires_grad)
    else:
        return sum(p.numel() for p in module.parameters())
    
def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']
 
def metrics_to_string(metric_dict):
    string_list = []
    for key, value in metric_dict.items():
        string_list.append('{}:{:.2f}'.format(key, value))
    return ' '.join(string_list)

# Set random seed
set_seed(19072021)

"""# Konfigurasi dan Load Pre-trained Model
Tahap selanjutnya adalah proses Load Model dan konfigurasi. Pada tahap ini, kita menggunakan pre-trained model Indobert-base-p1 yang memiliki 124.5 juta parameter. Untuk tipe Indobert lainnya, Anda dapat melihatnya pada tautan berikut. 

Model Indobert dibangun berdasarkan general-purpose architecture BERT (Bidirectional Encoder Representation from Transformers).  BERT didesain untuk membantu komputer memahami arti bahasa ambigu dalam teks. Caranya adalah menggunakan teks di sekitarnya untuk membangun konteks.
"""

# Load Tokenizer and Config
tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')
config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')
config.num_labels = DocumentSentimentDataset.NUM_LABELS
 
# Instantiate model
model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)

#melihat model
model

#menghitung parameter model
count_param(model)

"""# Persiapan Dataset
Di sini, kita akan menggnakan 2 kelas yang disediakan di PyTorch dalam modul torch.utils.data yaitu Dataset dan DataLoader. Disadur dari Pre-Trained Models for NLP Tasks Using PyTorch [39], kelas Dataset adalah sebuah abstract class yang perlu kita extend di PyTorch. Sedangkan, DataLoader adalah inti dari perangkat pemrosesan data di PyTorch. DataLoader menyediakan banyak fungsionalitas untuk mempersiapkan data termasuk berbagai metode sampling, komputasi paralel, dan pemrosesan terdistribusi. Nah, kita akan memindahkan objek dari kelas Dataset ke dalam objek dari kelas DataLoader untuk pemrosesan batch data lebih lanjut.
"""

train_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'
valid_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'
test_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'

'''
#isi dari class DocumentSentimentDataset yg sudah built in dengan packages
class DocumentSentimentDataset(Dataset):
    # Static constant variable
    LABEL2INDEX = {'positive': 0, 'neutral': 1, 'negative': 2} # Map dari label string ke index
    INDEX2LABEL = {0: 'positive', 1: 'neutral', 2: 'negative'} # Map dari Index ke label string
    NUM_LABELS = 3 # Jumlah label
   
    def load_dataset(self, path):
        df = pd.read_csv(path, sep='\t', header=None) # Baca tsv file dengan pandas
        df.columns = ['text','sentiment'] # Berikan nama pada kolom tabel
        df['sentiment'] = df['sentiment'].apply(lambda lab: self.LABEL2INDEX[lab]) # Konversi string label ke index
        return df
   
    def __init__(self, dataset_path, tokenizer, *args, **kwargs):
        self.data = self.load_dataset(dataset_path) # Load tsv file
 
        # Assign tokenizer, disini kita menggunakan tokenizer subword dari HuggingFace
        self.tokenizer = tokenizer 
 
    def __getitem__(self, index):
        data = self.data.loc[index,:] # Ambil data pada baris tertentu dari tabel
        text, sentiment = data['text'], data['sentiment'] # Ambil nilai text dan sentiment
        subwords = self.tokenizer.encode(text) # Tokenisasi text menjadi subword
    
    # Return numpy array dari subwords dan label
        return np.array(subwords), np.array(sentiment), data['text']
   
    def __len__(self):
        return len(self.data)  # Return panjang dari dataset


class DocumentSentimentDataLoader(DataLoader):
    def __init__(self, max_seq_len=512, *args, **kwargs):
        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)
        self.max_seq_len = max_seq_len # Assign batas maksimum subword
        self.collate_fn = self._collate_fn # Assign fungsi collate_fn dengan fungsi yang kita definisikan
       
    def _collate_fn(self, batch):
        batch_size = len(batch) # Ambil batch size
        max_seq_len = max(map(lambda x: len(x[0]), batch)) # Cari panjang subword maksimal dari batch 
        max_seq_len = min(self.max_seq_len, max_seq_len) # Bandingkan dengan batas yang kita tentukan sebelumnya
       
    # Buat buffer untuk subword, mask, dan sentiment labels, inisialisasikan semuanya dengan 0
        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)
        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)
        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)
       
    # Isi semua buffer
        for i, (subwords, sentiment, raw_seq) in enumerate(batch):
            subwords = subwords[:max_seq_len]
            subword_batch[i,:len(subwords)] = subwords
            mask_batch[i,:len(subwords)] = 1
            sentiment_batch[i,0] = sentiment
           
    # Return subword, mask, dan sentiment data
        return subword_batch, mask_batch, sentiment_batch
'''

#mendefinisikan variabel untuk dataset dan loader
train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)
valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)
test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)
 
train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)  
valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)  
test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)

print(train_dataset[0])

w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL
print(w2i)
print(i2w)

#uji model dengan contoh kalimat
text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'
subwords = tokenizer.encode(text)
subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)
 
logits = model(subwords)[0]
label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()
 
print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')

'''
Padahal, kita dapat mengidentifikasi kalau teks tersebut seharusnya masuk dalam kategori sentimen positif. Oleh karena itu, mari kita lakukan proses Fine Tuning dan Evaluasi.
'''

'''
Fine Tuning dan Evaluasi
Pertama, definisikan optimizer. Di sini kita menggunakan Adam optimizer pada library PyTorch. 
Selanjutnya, jika Anda menjalankan proyek di Google Colab, pastikan pengaturan runtime telah diubah ke GPU. 
Anda dapat mengubah pengaturannya pada menu toolbar. Klik Runtime > Change Runtime Type > Hardware Accelerator pilih GPU, lalu klik Save. 
'''
#Sekarang, jalankan kode berikut. Panggil juga fungsi cuda() agar proses pelatihan berjalan cepat dengan bantuan gpu. 
optimizer = optim.Adam(model.parameters(), lr=3e-6)
model = model.cuda()

'''
Selanjutnya, kita akan melatih model dengan jumlah epoch = 5. Tahapan yang akan kita lakukan, antara lain:

1. Proses pelatihan dan pembaruan model
2. Kalkulasi metriks pelatihan
3. Evaluasi pada data validasi 
4. Kalkulasi matriks validasi
'''

# Train
n_epochs = 5
for epoch in range(n_epochs):
    model.train()
    torch.set_grad_enabled(True)
 
    total_train_loss = 0
    list_hyp, list_label = [], []
 
    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))
    for i, batch_data in enumerate(train_pbar):
        # Forward model
        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')
 
        # Update model
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
 
        tr_loss = loss.item()
        total_train_loss = total_train_loss + tr_loss
 
        # Calculate metrics
        list_hyp += batch_hyp
        list_label += batch_label
 
        train_pbar.set_description("(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}".format((epoch+1),
            total_train_loss/(i+1), get_lr(optimizer)))
 
    # Calculate train metric
    metrics = document_sentiment_metrics_fn(list_hyp, list_label)
    print("(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}".format((epoch+1),
        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))
 
    # Evaluate on validation
    model.eval()
    torch.set_grad_enabled(False)
    
    total_loss, total_correct, total_labels = 0, 0, 0
    list_hyp, list_label = [], []
 
    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))
    for i, batch_data in enumerate(pbar):
        batch_seq = batch_data[-1]        
        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')
        
        # Calculate total loss
        valid_loss = loss.item()
        total_loss = total_loss + valid_loss
 
        # Calculate evaluation metrics
        list_hyp += batch_hyp
        list_label += batch_label
        metrics = document_sentiment_metrics_fn(list_hyp, list_label)
 
        pbar.set_description("VALID LOSS:{:.4f} {}".format(total_loss/(i+1), metrics_to_string(metrics)))
        
    metrics = document_sentiment_metrics_fn(list_hyp, list_label)
    print("(Epoch {}) VALID LOSS:{:.4f} {}".format((epoch+1),
        total_loss/(i+1), metrics_to_string(metrics)))

# Evaluate on test
model.eval()
torch.set_grad_enabled(False)
 
total_loss, total_correct, total_labels = 0, 0, 0
list_hyp, list_label = [], []
 
pbar = tqdm(test_loader, leave=True, total=len(test_loader))
for i, batch_data in enumerate(pbar):
    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')
    list_hyp += batch_hyp
 
# Save prediction
df = pd.DataFrame({'label':list_hyp}).reset_index()
df.to_csv('pred.txt', index=False)
 
print(df)
# Evaluate on test
model.eval()
torch.set_grad_enabled(False)
 
total_loss, total_correct, total_labels = 0, 0, 0
list_hyp, list_label = [], []
 
pbar = tqdm(test_loader, leave=True, total=len(test_loader))
for i, batch_data in enumerate(pbar):
    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')
    list_hyp += batch_hyp
 
# Save prediction
df = pd.DataFrame({'label':list_hyp}).reset_index()
df.to_csv('pred.txt', index=False)
 
print(df)

'''
Prediksi Sentimen
Sekarang, saatnya kita menguji model pada beberapa contoh kalimat. 
Jalankan kode berikut dan ubah variabel text dengan beberapa contoh kalimat yang Anda sukai.

“Ronaldo pergi ke Mall Grand Indonesia membeli cilok”
“Sayang, aku marah”
“Merasa kagum dengan toko ini tapi berubah menjadi kecewa setelah transaksi”
Kalimat nomor dua sengaja dibuat seperti itu dengan maksud untuk mengecoh mesin.
 Kata “sayang” biasanya berkonotasi positif. Tapi dalam kalimat tersebut diikuti oleh kata “marah”. 

Demikian juga dengan kalimat nomor 3, “kagum” dan “kecewa” adalah dua kata yang masing-masing berkonotasi positif dan negatif. 
Jika dilihat dari konteks kalimatnya, jelas kedua kalimat ini bersentimen negatif. 
Manusia bisa langsung mengenalinya sebagai sentimen negatif. 
Namun, bagaimana dengan mesin yang telah dilatih dengan model kita?
'''

text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'
subwords = tokenizer.encode(text)
subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)
 
logits = model(subwords)[0]
label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()
 
print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')

# Commented out IPython magic to ensure Python compatibility.
'''
Implementasi SVM untuk Analisis Sentimen pada Data IndoNLU
Sekarang, mari kita implementasikan SVM untuk analisis sentimen pada data IndoNLU. Berikut beberapa hal yang akan kita lakukan pada materi ini:

Membaca Data
Analisis Data
Feature Engineering dengan TF-IDF
Klasifikasi Sentimen dengan SVM
Prediksi Sentimen
'''

import pandas as pd
 
# %matplotlib inline
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn import svm
from sklearn.metrics import classification_report

#mengambil data dari github
!git clone https://github.com/indobenchmark/indonlu

#membaca data
data_train = pd.read_csv('/content/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv', sep='\t')
data_train.head()

'''
Jika outputnya diperhatikan kembali, data tidak memiliki header. 
Sehingga, kita perlu membuat header untuk data dan nama kolom sekaligus untuk membedakan antara konten teks dengan target label. 
Jalankan kode berikut.
'''

data_train = pd.read_csv('/content/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv', sep='\t', names=["Teks", "Target"])
data_test = pd.read_csv('/content/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv', sep='\t', names=["Teks", "Target"])
 
#mengambil 5 data teratas pada data train
data_train.head(5)

'''
Analisis Data
Sebelum masuk ke pemodelan, kita perlu melakukan tahap analisis data terlebih dahulu. 
Pada tahap ini, kita akan mengecek berapa jumlah data_train dan data_test, 
komposisi label atau target sentimen pada data, dan menggunakan fungsi world cloud untuk melihat term yang muncul pada data.
'''

#untuk melihat jumlah data pada data train
 
print(data_train.shape)
print(data_test.shape)

'''
Selanjutnya, untuk menghitung jumlah masing-masing variabel (positif, netral, dan negatif) pada kolom target data_train,
'''

#menghitung jumlah variabel pada kolom Target
data_train['Target'].value_counts()

'''
Mari kita visualisasikan data ini untuk mendapatkan ilustrasi distribusi variabel dengan lebih jelas
'''

data_train.groupby('Target').size().plot(kind='bar')


'''
Terlihat dari grafik kita memiliki dataset yang tidak seimbang (imbalanced dataset). Meskipun demikian, 
ini tidak menjadi masalah yang signifikan karena kita melatih ulasan berlabel sentimen satu per satu, 
menggunakan label sentimen masing-masing ulasan.
'''

#melihat panjang teks dalam data train dan data test
 
length_train = data_train['Teks'].str.len()
length_test = data_test['Teks'].str.len()
plt.figure(figsize=(10,6))
plt.hist(length_train, bins=50, label="Train_Teks", color = "darkblue")
plt.hist(length_test, bins=50, label='Test_Teks', color = "skyblue")
plt.legend()

'''
Untuk melihat sentimen pada data, kadang kita perlu melakukan visualisasi terhadap kata-kata dengan fungsi word cloud. 
Caranya adalah dengan mendefinisikan fungsinya terlebih dahulu. 
Kode berikut untuk membuat visualisasi wordcloud pada data_train. 
Jika Anda ingin melakukannya juga pada data_test, Anda tinggal mengubah sedikit kodenya.
'''

# Sentimen yang ada pada seluruh data train dapat dilihat dengan memahami kata yang umum dengan plot word cloud
 
def wordCloud(words):
    wordCloud = WordCloud(width=800, height=500, background_color='white', random_state=21, max_font_size=120).generate(words)
    
    plt.figure(figsize=(10, 7))
    plt.imshow(wordCloud, interpolation='bilinear')
    plt.axis('off')
 
all_words = ' '.join([text for text in data_train['Teks']])
wordCloud(all_words)

#untuk melihat kata kata yang negatif
negative_words = ' '.join(text for text in data_train['Teks'][data_train['Target'] == 'negative']) 
wordCloud(negative_words)

# melakukan vektorisasi untuk mengekstrak fitur dengan TF-IDF
vectorizer = TfidfVectorizer(min_df = 5, #Digunakan untuk menghilangkan term/istilah yang terlalu jarang muncul (kurang dari 5 dokumen)
                             max_df = 0.8, #Digunakan untuk menghilangkan term yang terlalu sering muncul, biasanya merupakan stop words
                                            #mengabaikan term yang muncul lebih dari 80% dalam teks
                             sublinear_tf = True, #Berfungsi untuk melakukan scaling dan bernilai boolean dengan default false. Sublinear_tf=True akan mengubah vektor frekuensi menjadi bentuk logaritmik (1+log(tf)). Sehingga dapat menormalisasi bias terhadap teks yang panjang dan teks yang pendek.
                             use_idf = True) #Bernilai boolean dengan default True. Parameter ini memungkinkan kita untuk menggunakan Inverse Document Frequency (IDF). Hal ini berarti term yang terlalu sering muncul dalam teks akan diberi skor lebih sedikit dibanding term yang jarang muncul (hanya muncul pada teks yang spesifik saja).
 
#
train_vectors = vectorizer.fit_transform(data_train['Teks'])
test_vectors = vectorizer.transform(data_test['Teks'])

# melakukan klasifikasi dengan SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
classifier_linear.fit(train_vectors, data_train['Target'])
prediction_linear = classifier_linear.predict(test_vectors)

# print metriks
target_names = ['positive', 'negative', 'netral']
print(classification_report(data_test['Target'], prediction_linear, target_names=target_names))
'''
Hasil metriknya memang tidak lebih bagus dari teknik Deep Learning sebelumnya, 
tapi masih cukup bagus dan nilainya konsisten. Selanjutnya, mari kita coba untuk memprediksi beberapa teks masukan.
'''

teks = """Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita"""
teks_vector = vectorizer.transform([teks]) # vectorizing
print(classifier_linear.predict(teks_vector))